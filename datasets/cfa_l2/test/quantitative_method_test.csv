id,question,A,B,C,D
0,"Aaliyah Schultz is a fixed-income portfolio manager at Aries Investments. Schultz supervises Ameris Steele, a junior analyst.A few years ago, Schultz developed a proprietary machine learning (ML) model that aims to predict downgrades of publicly-traded firms by bond rating agencies. The model currently relies only on structured financial data collected from different sources. Schultz thinks the model’s predictive power may be improved by incorporat-ing sentiment data derived from textual analysis of news articles and Twitter content relating to the subject companies.Schultz and Steele meet to discuss plans for incorporating the sentiment data into the model. They discuss the differences in the steps between building ML models that use traditional structured data and building ML models that use textual big data. Steele tells Schultz:Statement 1 The second step in building text-based ML models is text prepa-ration and wrangling, whereas the second step in building ML models using structured data is data collection.Statement 2 The fourth step in building both types of models encompasses data/text exploration.Steele expresses concern about using Twitter content in the model, noting that research suggests that as much as 10%–15% of social media content is from fake accounts. Schultz tells Steele that she understands her concern but thinks the potential for model improvement outweighs the concern.Steele begins building a model that combines the structured financial data and the sentiment data. She starts with cleansing and wrangling the raw structured financial data. Exhibit 1 presents a small sample of the raw dataset before cleansing: Each row represents data for a particular firm.
| Exhibit 1Sample of Raw Structured Data Before Cleansing |
| Industry | Interest |
| ID | Ticker | IPO Date | (NAICS) | EBIT | Expense | Total Debt |
| 1 | ABC | 4/6/17 | 44 | 9.4 | 0.6 | 10.1 |
| 2 | BCD | November 15， 2004 | 52 | 5.5 | 0.4 | 6.2 |
| 3 | HIJ | 26-Jun-74 | 54 | 8.9 | 1.2 | 15.8 |
| 4 | KLM | 14-Mar-15 | 72 | 5.7 | 1.5 | 0.0 |


After cleansing the data, Steele then preprocesses the dataset. She creates two new variables: an “Age” variable based on the firm’s IPO date and an “Interest Coverage Ratio” variable equal to EBIT divided by interest expense. She also deletes the “IPO Date” variable from the dataset. After applying these transformations, Steele scales the financial data using normalization. She notes that over the full sample dataset, the “Interest Expense” variable ranges from a minimum of 0.2 and a maximum of 12.2, with a mean of 1.1 and a standard deviation of 0.4.Steele and Schultz then discuss how to preprocess the raw text data. Steele tells Schultz that the process can be completed in the following three steps:Step 1 Cleanse the raw text data.Step 2 Split the cleansed data into a collection of words for them to be normalized.Step 3 Normalize the collection of words from Step 2 and create a distinct set of tokens from the normalized words.With respect to Step 1, Steele tells Schultz:“I believe I should remove all html tags, punctuations, numbers, and extra white spaces from the data before normalizing them.”After properly cleansing the raw text data, Steele completes Steps 2 and 3. She then performs exploratory data analysis. To assist in feature selection, she wants to create a visualization that shows the most informative words in the dataset based on their term frequency (TF) values. After creating and analyzing the visualization, Steele is concerned that some tokens are likely to be noise features for ML model training; therefore, she wants to remove them.Steele and Schultz discuss the importance of feature selection and feature engi-neering in ML model training. Steele tells Schultz:“Appropriate feature selection is a key factor in minimizing model over-fitting, whereas feature engineering tends to prevent model underfitting.”Once satisfied with the final set of features, Steele selects and runs a model on the training set that classifies the text as having positive sentiment (Class “1” or negative sentiment (Class “0”). She then evaluates its performance using error analysis. The resulting confusion matrix is presented in Exhibit 2.
|  |  | Actual Training Results |
|  |  | Class""1"" | Class“O"" |
| PredictedResults | Class“1"" | TP=182 | FP=52 |
| Class“0"" | FN=31 | TN=96 |


 
Steele’s Step 2 can be best described as:",tokenization.,lemmatization.,standardization.,
